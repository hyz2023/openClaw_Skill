#!/usr/bin/env python3
"""
é˜¿é‡Œäº‘ ODPS/MaxCompute æŸ¥è¯¢å·¥å…·

åŠŸèƒ½:
- åˆ—å‡ºé¡¹ç›®ä¸­çš„æ‰€æœ‰è¡¨
- æŸ¥çœ‹è¡¨ç»“æ„å’Œå…ƒæ•°æ®
- æ‰§è¡Œ SQL æŸ¥è¯¢
- å¯¼å‡ºæŸ¥è¯¢ç»“æœ

ä½¿ç”¨ç¤ºä¾‹:
    python scripts/odps_query.py --action list --project my_project
    python scripts/odps_query.py --action describe --project my_project --table user_info
    python scripts/odps_query.py --action query --project my_project --sql "SELECT * FROM table LIMIT 10"
"""

import argparse
import json
import os
import sys
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path

try:
    from odps import ODPS
    from odps.models import Table
except ImportError:
    print("è¯·å®‰è£… ODPS åº“ï¼špip install odps")
    sys.exit(1)

try:
    import pandas as pd
except ImportError:
    print("è¯·å®‰è£… pandas: pip install pandas")
    pandas = None


class ODPSQuery:
    """ODPS æŸ¥è¯¢å·¥å…·ç±»"""
    
    def __init__(
        self,
        access_id: Optional[str] = None,
        access_key: Optional[str] = None,
        endpoint: Optional[str] = None,
        project: Optional[str] = None
    ):
        """
        åˆå§‹åŒ– ODPS è¿æ¥
        
        Args:
            access_id: é˜¿é‡Œäº‘ AccessKey ID
            access_key: é˜¿é‡Œäº‘ AccessKey Secret
            endpoint: ODPS æœåŠ¡ç«¯ç‚¹
            project: ODPS é¡¹ç›®åç§°
        """
        # ä»å‚æ•°æˆ–ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.access_id = access_id or os.getenv('ALIBABA_ACCESSKEY_ID')
        self.access_key = access_key or os.getenv('ALIBABA_ACCESSKEY_SECRET')
        self.endpoint = endpoint or os.getenv('ALIBABA_ODPS_ENDPOINT', 'http://service.odps.aliyun.com/api')
        self.project = project or os.getenv('ALIBABA_ODPS_PROJECT')
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not self.access_id:
            raise ValueError("ç¼ºå°‘ AccessKey IDï¼Œè¯·è®¾ç½® ALIBABA_ACCESSKEY_ID ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨ --access-id å‚æ•°")
        if not self.access_key:
            raise ValueError("ç¼ºå°‘ AccessKey Secretï¼Œè¯·è®¾ç½® ALIBABA_ACCESSKEY_SECRET ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨ --access-key å‚æ•°")
        if not self.project:
            raise ValueError("ç¼ºå°‘é¡¹ç›®åç§°ï¼Œè¯·ä½¿ç”¨ --project å‚æ•°æˆ–è®¾ç½® ALIBABA_ODPS_PROJECT ç¯å¢ƒå˜é‡")
        
        # åˆå§‹åŒ– ODPS å®¢æˆ·ç«¯
        self.client = ODPS(
            access_id=self.access_id,
            secret_access_key=self.access_key,
            endpoint=self.endpoint,
            project=self.project
        )
        
    def list_tables(self, pattern: Optional[str] = None) -> List[Dict]:
        """
        åˆ—å‡ºé¡¹ç›®ä¸­çš„æ‰€æœ‰è¡¨
        
        Args:
            pattern: è¡¨ååŒ¹é…æ¨¡å¼ (æ”¯æŒé€šé…ç¬¦)
            
        Returns:
            è¡¨ä¿¡æ¯åˆ—è¡¨
        """
        tables = []
        
        try:
            for table in self.client.list_tables():
                table_info = {
                    'name': table.name,
                    'project': table.project,
                    'created_time': str(table.creation_time) if hasattr(table, 'creation_time') else 'N/A',
                    'is_virtual_view': getattr(table, 'is_virtual_view', False),
                }
                
                # å°è¯•è·å–è¡¨å¤§å°
                try:
                    table.reload()
                    table_info['size'] = getattr(table, 'size', 0)
                except:
                    table_info['size'] = 0
                
                # è¿‡æ»¤
                if pattern and pattern.lower() not in table.name.lower():
                    continue
                
                tables.append(table_info)
            
        except Exception as e:
            print(f"âŒ è·å–è¡¨åˆ—è¡¨å¤±è´¥ï¼š{e}")
            return []
        
        # æŒ‰è¡¨åæ’åº
        tables.sort(key=lambda x: x['name'])
        return tables
    
    def describe_table(self, table_name: str) -> Dict:
        """
        æŸ¥çœ‹è¡¨ç»“æ„
        
        Args:
            table_name: è¡¨å
            
        Returns:
            è¡¨ç»“æ„ä¿¡æ¯
        """
        try:
            table = self.client.get_table(table_name)
            table.reload()
            
            # è·å–å­—æ®µä¿¡æ¯
            schema = table.schema
            columns = []
            
            for col in schema.columns:
                col_info = {
                    'name': col.name,
                    'type': str(col.type),
                    'comment': getattr(col, 'comment', ''),
                    'label': getattr(col, 'label', ''),
                }
                columns.append(col_info)
            
            # è·å–åˆ†åŒºä¿¡æ¯
            partitions = []
            if schema.partitions:
                for pt in schema.partitions:
                    partitions.append({
                        'name': pt.name,
                        'type': str(pt.type),
                        'comment': getattr(pt, 'comment', ''),
                    })
            
            # è¡¨åŸºæœ¬ä¿¡æ¯
            table_info = {
                'name': table.name,
                'project': table.project,
                'comment': getattr(table, 'comment', ''),
                'created_time': str(table.creation_time) if hasattr(table, 'creation_time') else 'N/A',
                'last_modified_time': str(table.last_data_modified_time) if hasattr(table, 'last_data_modified_time') else 'N/A',
                'size': getattr(table, 'size', 0),
                'lifecycle': getattr(table, 'lifecycle', 0),
                'is_virtual_view': getattr(table, 'is_virtual_view', False),
                'columns': columns,
                'partitions': partitions,
            }
            
            return table_info
            
        except Exception as e:
            return {'error': str(e)}
    
    def execute_query(
        self,
        sql: str,
        limit: int = 100,
        project: Optional[str] = None
    ) -> Dict:
        """
        æ‰§è¡Œ SQL æŸ¥è¯¢
        
        Args:
            sql: SQL è¯­å¥
            limit: ç»“æœè¡Œæ•°é™åˆ¶
            project: é¡¹ç›®åç§° (å¯é€‰ï¼Œè¦†ç›–é»˜è®¤é¡¹ç›®)
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        # éªŒè¯ SQL (ä»…å…è®¸ SELECT)
        sql_upper = sql.strip().upper()
        if not sql_upper.startswith('SELECT'):
            return {'error': 'ä»…æ”¯æŒ SELECT æŸ¥è¯¢è¯­å¥'}
        
        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            query_project = project or self.project
            instance = self.client.execute_sql(sql, project=query_project)
            
            # ç­‰å¾…æŸ¥è¯¢å®Œæˆ
            instance.wait_for_success()
            
            # è·å–ç»“æœ
            with self.client.open_reader(instance.id) as reader:
                # è·å–åˆ—å
                columns = [col.name for col in reader.schema.columns]
                
                # è·å–æ•°æ®
                rows = []
                count = 0
                for record in reader:
                    if limit and count >= limit:
                        break
                    row = {columns[i]: record[i] for i in range(len(columns))}
                    rows.append(row)
                    count += 1
                
                return {
                    'success': True,
                    'columns': columns,
                    'data': rows,
                    'count': len(rows),
                    'sql': sql,
                }
                
        except Exception as e:
            return {'error': str(e), 'sql': sql}
    
    def export_results(
        self,
        data: List[Dict],
        output_format: str = 'csv',
        output_file: Optional[str] = None
    ) -> Optional[str]:
        """
        å¯¼å‡ºæŸ¥è¯¢ç»“æœ
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            output_format: è¾“å‡ºæ ¼å¼ (csv/json/excel)
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„æˆ–å†…å®¹
        """
        if not data:
            return None
        
        if output_format == 'json':
            content = json.dumps(data, indent=2, ensure_ascii=False, default=str)
            if output_file:
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                return output_file
            return content
        
        elif output_format == 'csv' and pandas is not None:
            df = pd.DataFrame(data)
            if output_file:
                df.to_csv(output_file, index=False, encoding='utf-8-sig')
                return output_file
            return df.to_csv(index=False, encoding='utf-8-sig')
        
        elif output_format == 'excel' and pandas is not None:
            df = pd.DataFrame(data)
            if not output_file:
                output_file = f"odps_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            df.to_excel(output_file, index=False, engine='openpyxl')
            return output_file
        
        else:
            return str(data)


def format_table_list(tables: List[Dict]) -> str:
    """æ ¼å¼åŒ–è¡¨åˆ—è¡¨è¾“å‡º"""
    if not tables:
        return "æœªæ‰¾åˆ°ä»»ä½•è¡¨"
    
    lines = []
    lines.append(f"ğŸ“Š é¡¹ç›®ï¼š{tables[0].get('project', 'N/A')}")
    lines.append(f"æ‰¾åˆ° {len(tables)} å¼ è¡¨\n")
    lines.append(f"{'è¡¨å':<40} {'åˆ›å»ºæ—¶é—´':<20} {'å¤§å° (MB)':<12}")
    lines.append("-" * 72)
    
    for table in tables:
        name = table.get('name', 'N/A')[:38]
        created = table.get('created_time', 'N/A')[:19] if table.get('created_time') else 'N/A'
        size_mb = table.get('size', 0) / (1024 * 1024)
        size_str = f"{size_mb:.2f}" if size_mb > 0 else "N/A"
        
        lines.append(f"{name:<40} {created:<20} {size_str:<12}")
    
    return "\n".join(lines)


def format_table_schema(schema: Dict) -> str:
    """æ ¼å¼åŒ–è¡¨ç»“æ„è¾“å‡º"""
    if 'error' in schema:
        return f"âŒ é”™è¯¯ï¼š{schema['error']}"
    
    lines = []
    lines.append(f"ğŸ“‹ è¡¨ç»“æ„ï¼š{schema.get('project', 'N/A')}.{schema.get('name', 'N/A')}")
    
    if schema.get('comment'):
        lines.append(f"æ³¨é‡Šï¼š{schema['comment']}")
    
    lines.append(f"\nåŸºæœ¬ä¿¡æ¯:")
    lines.append(f"  åˆ›å»ºæ—¶é—´ï¼š{schema.get('created_time', 'N/A')}")
    lines.append(f"  æœ€åä¿®æ”¹ï¼š{schema.get('last_modified_time', 'N/A')}")
    size_mb = schema.get('size', 0) / (1024 * 1024)
    lines.append(f"  è¡¨å¤§å°ï¼š{size_mb:.2f} MB" if size_mb > 0 else "  è¡¨å¤§å°ï¼šN/A")
    lines.append(f"  ç”Ÿå‘½å‘¨æœŸï¼š{schema.get('lifecycle', 0)} å¤©" if schema.get('lifecycle') else "  ç”Ÿå‘½å‘¨æœŸï¼šæ°¸ä¹…")
    
    # å­—æ®µä¿¡æ¯
    columns = schema.get('columns', [])
    if columns:
        lines.append(f"\nå­—æ®µ ({len(columns)}åˆ—):")
        lines.append(f"{'å­—æ®µå':<30} {'ç±»å‹':<20} {'æ³¨é‡Š':<30}")
        lines.append("-" * 80)
        
        for col in columns:
            name = col.get('name', 'N/A')[:28]
            col_type = col.get('type', 'N/A')[:18]
            comment = col.get('comment', '')[:28]
            lines.append(f"{name:<30} {col_type:<20} {comment:<30}")
    
    # åˆ†åŒºä¿¡æ¯
    partitions = schema.get('partitions', [])
    if partitions:
        lines.append(f"\nåˆ†åŒº ({len(partitions)}åˆ—):")
        lines.append(f"{'åˆ†åŒºå':<30} {'ç±»å‹':<20} {'æ³¨é‡Š':<30}")
        lines.append("-" * 80)
        
        for pt in partitions:
            name = pt.get('name', 'N/A')[:28]
            pt_type = pt.get('type', 'N/A')[:18]
            comment = pt.get('comment', '')[:28]
            lines.append(f"{name:<30} {pt_type:<20} {comment:<30}")
    
    return "\n".join(lines)


def format_query_result(result: Dict) -> str:
    """æ ¼å¼åŒ–æŸ¥è¯¢ç»“æœè¾“å‡º"""
    if 'error' in result:
        return f"âŒ æŸ¥è¯¢å¤±è´¥ï¼š{result['error']}\nSQL: {result.get('sql', 'N/A')}"
    
    if not result.get('success'):
        return "âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥"
    
    lines = []
    lines.append(f"âœ… æŸ¥è¯¢æˆåŠŸ")
    lines.append(f"è¿”å› {result.get('count', 0)} è¡Œ\n")
    
    # è¡¨æ ¼å½¢å¼æ˜¾ç¤º
    data = result.get('data', [])
    if data:
        columns = result.get('columns', [])
        
        # è®¡ç®—åˆ—å®½
        col_widths = {}
        for col in columns:
            col_widths[col] = len(col)
            for row in data:
                val_len = len(str(row.get(col, '')))
                col_widths[col] = max(col_widths[col], val_len)
        
        # è¡¨å¤´
        header = " | ".join(col.ljust(col_widths[col]) for col in columns)
        lines.append(header)
        lines.append("-" * len(header))
        
        # æ•°æ®è¡Œ
        for row in data:
            row_str = " | ".join(str(row.get(col, '')).ljust(col_widths[col]) for col in columns)
            lines.append(row_str)
    
    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description='é˜¿é‡Œäº‘ ODPS æŸ¥è¯¢å·¥å…·')
    parser.add_argument('--action', type=str, required=True,
                        choices=['list', 'describe', 'query'],
                        help='æ“ä½œç±»å‹ï¼šlist(åˆ—è¡¨)/describe(æè¿°)/query(æŸ¥è¯¢)')
    parser.add_argument('--project', type=str, required=True,
                        help='ODPS é¡¹ç›®åç§°')
    parser.add_argument('--table', type=str, default=None,
                        help='è¡¨å (describe æ“ä½œéœ€è¦)')
    parser.add_argument('--sql', type=str, default=None,
                        help='SQL è¯­å¥ (query æ“ä½œéœ€è¦)')
    parser.add_argument('--limit', type=int, default=100,
                        help='ç»“æœè¡Œæ•°é™åˆ¶ (é»˜è®¤ 100)')
    parser.add_argument('--output', type=str, default='table',
                        choices=['table', 'csv', 'json', 'excel'],
                        help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--output-file', type=str, default=None,
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--access-id', type=str, default=None,
                        help='AccessKey ID (è¦†ç›–ç¯å¢ƒå˜é‡)')
    parser.add_argument('--access-key', type=str, default=None,
                        help='AccessKey Secret (è¦†ç›–ç¯å¢ƒå˜é‡)')
    parser.add_argument('--endpoint', type=str, default=None,
                        help='ODPS Endpoint (è¦†ç›–ç¯å¢ƒå˜é‡)')
    parser.add_argument('--pattern', type=str, default=None,
                        help='è¡¨ååŒ¹é…æ¨¡å¼ (list æ“ä½œä½¿ç”¨)')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ– ODPS è¿æ¥
    try:
        odps = ODPSQuery(
            access_id=args.access_id,
            access_key=args.access_key,
            endpoint=args.endpoint,
            project=args.project
        )
        print(f"âœ… å·²è¿æ¥åˆ° ODPS é¡¹ç›®ï¼š{args.project}\n")
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥ï¼š{e}")
        sys.exit(1)
    
    # æ‰§è¡Œæ“ä½œ
    if args.action == 'list':
        tables = odps.list_tables(pattern=args.pattern)
        output = format_table_list(tables)
        print(output)
        
    elif args.action == 'describe':
        if not args.table:
            print("âŒ describe æ“ä½œéœ€è¦æŒ‡å®š --table å‚æ•°")
            sys.exit(1)
        
        schema = odps.describe_table(args.table)
        output = format_table_schema(schema)
        print(output)
        
    elif args.action == 'query':
        if not args.sql:
            print("âŒ query æ“ä½œéœ€è¦æŒ‡å®š --sql å‚æ•°")
            sys.exit(1)
        
        result = odps.execute_query(args.sql, limit=args.limit, project=args.project)
        
        # å¯¼å‡ºç»“æœ
        if args.output in ['csv', 'json', 'excel'] and result.get('success'):
            output_file = args.output_file or f"odps_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{args.output}"
            saved_file = odps.export_results(
                result.get('data', []),
                output_format=args.output,
                output_file=output_file
            )
            print(f"âœ… ç»“æœå·²å¯¼å‡ºåˆ°ï¼š{saved_file}")
        else:
            output = format_query_result(result)
            print(output)


if __name__ == "__main__":
    main()
