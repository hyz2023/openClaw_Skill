#!/usr/bin/env python3
"""
ODPS å…ƒæ•°æ®é‡‡é›†å·¥å…·
é‡‡é›†æ‰€æœ‰è¡¨ç»“æ„ã€å­—æ®µä¿¡æ¯ã€åˆ†åŒºä¿¡æ¯ï¼Œä¿å­˜åˆ°æœ¬åœ°

ç¯å¢ƒå˜é‡:
export ALIBABA_ACCESSKEY_ID="your_access_key_id"
export ALIBABA_ACCESSKEY_SECRET="your_access_key_secret"
export ALIBABA_ODPS_PROJECT="superengineproject"
export ALIBABA_ODPS_ENDPOINT="http://service.ap-southeast-1.maxcompute.aliyun.com/api"
"""

import os
import json
from datetime import datetime
from pathlib import Path
from odps import ODPS
import pandas as pd


def connect_odps():
    """è¿æ¥ ODPS"""
    access_id = os.getenv('ALIBABA_ACCESSKEY_ID')
    access_key = os.getenv('ALIBABA_ACCESSKEY_SECRET')
    project = os.getenv('ALIBABA_ODPS_PROJECT', 'superengineproject')
    endpoint = os.getenv('ALIBABA_ODPS_ENDPOINT', 'http://service.ap-southeast-1.maxcompute.aliyun.com/api')
    
    print(f"è¿æ¥ ODPS é¡¹ç›®ï¼š{project}")
    
    return ODPS(
        access_id=access_id,
        secret_access_key=access_key,
        project=project,
        endpoint=endpoint
    )


def list_all_tables(o: ODPS) -> list:
    """åˆ—å‡ºæ‰€æœ‰è¡¨"""
    print("\nğŸ“‹ è·å–è¡¨åˆ—è¡¨...")
    tables = []
    
    for table in o.list_tables():
        tables.append(table.name)
    
    print(f"æ‰¾åˆ° {len(tables)} å¼ è¡¨")
    return tables


def get_table_metadata(o: ODPS, table_name: str) -> dict:
    """è·å–å•å¼ è¡¨çš„è¯¦ç»†å…ƒæ•°æ®"""
    try:
        table = o.get_table(table_name)
        
        # åŸºæœ¬ä¿¡æ¯
        metadata = {
            'table_name': table.name,
            'comment': getattr(table, 'comment', ''),
            'create_time': str(table.creation_time) if hasattr(table, 'creation_time') else None,
            'last_modified_time': str(table.last_modified_time) if hasattr(table, 'last_modified_time') else None,
            'size': table.size if hasattr(table, 'size') else None,
            'is_virtual_view': getattr(table, 'is_virtual_view', False),
            'lifecycle': getattr(table, 'lifecycle', None)
        }
        
        # å­—æ®µä¿¡æ¯
        columns = []
        for col in table.table_schema.columns:
            col_info = {
                'name': col.name,
                'type': str(col.type),
                'comment': getattr(col, 'comment', ''),
                'is_nullable': getattr(col, 'is_nullable', True)
            }
            columns.append(col_info)
        
        metadata['columns'] = columns
        metadata['column_count'] = len(columns)
        
        # åˆ†åŒºä¿¡æ¯
        partitions = []
        if table.table_schema.partitions:
            for pt in table.table_schema.partitions:
                pt_info = {
                    'name': pt.name,
                    'type': str(pt.type),
                    'comment': getattr(pt, 'comment', '')
                }
                partitions.append(pt_info)
        
        metadata['partitions'] = partitions
        metadata['is_partitioned'] = len(partitions) > 0
        
        return metadata
        
    except Exception as e:
        print(f"âš ï¸  è·å–è¡¨ {table_name} å…ƒæ•°æ®å¤±è´¥ï¼š{e}")
        return None


def crawl_all_metadata(o: ODPS, output_dir: str = 'odps_metadata'):
    """é‡‡é›†æ‰€æœ‰è¡¨çš„å…ƒæ•°æ®"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è·å–æ‰€æœ‰è¡¨
    table_names = list_all_tables(o)
    
    all_metadata = []
    failed_tables = []
    
    # é€è¡¨é‡‡é›†
    for i, table_name in enumerate(table_names, 1):
        print(f"[{i}/{len(table_names)}] é‡‡é›† {table_name}...")
        
        metadata = get_table_metadata(o, table_name)
        
        if metadata:
            all_metadata.append(metadata)
        else:
            failed_tables.append(table_name)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # 1. JSON æ ¼å¼ (å®Œæ•´å…ƒæ•°æ®)
    json_file = output_path / f'metadata_{timestamp}.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            'project': os.getenv('ALIBABA_ODPS_PROJECT'),
            'crawl_time': timestamp,
            'table_count': len(all_metadata),
            'failed_tables': failed_tables,
            'tables': all_metadata
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å®Œæ•´å…ƒæ•°æ®å·²ä¿å­˜ï¼š{json_file}")
    
    # 2. CSV æ ¼å¼ (å­—æ®µæ¸…å•)
    all_columns = []
    for table in all_metadata:
        for col in table['columns']:
            all_columns.append({
                'table_name': table['table_name'],
                'column_name': col['name'],
                'column_type': col['type'],
                'comment': col['comment'],
                'is_nullable': col['is_nullable']
            })
    
    csv_file = output_path / f'columns_{timestamp}.csv'
    df = pd.DataFrame(all_columns)
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    
    print(f"âœ… å­—æ®µæ¸…å•å·²ä¿å­˜ï¼š{csv_file}")
    
    # 3. åˆ›å»ºæœ€æ–°å…ƒæ•°æ®çš„ç¬¦å·é“¾æ¥
    latest_json = output_path / 'metadata_latest.json'
    latest_csv = output_path / 'columns_latest.csv'
    
    if latest_json.exists():
        latest_json.unlink()
    if latest_csv.exists():
        latest_csv.unlink()
    
    # å¤åˆ¶æ–‡ä»¶ä½œä¸º latest
    import shutil
    shutil.copy(json_file, latest_json)
    shutil.copy(csv_file, latest_csv)
    
    print(f"âœ… æœ€æ–°å…ƒæ•°æ®é“¾æ¥å·²åˆ›å»º")
    
    # 4. ç»Ÿè®¡æ‘˜è¦
    summary = {
        'project': os.getenv('ALIBABA_ODPS_PROJECT'),
        'crawl_time': timestamp,
        'total_tables': len(all_metadata),
        'total_columns': sum(t['column_count'] for t in all_metadata),
        'partitioned_tables': sum(1 for t in all_metadata if t['is_partitioned']),
        'failed_tables': failed_tables,
        'top_tables_by_columns': sorted(
            [(t['table_name'], t['column_count']) for t in all_metadata],
            key=lambda x: x[1],
            reverse=True
        )[:10]
    }
    
    summary_file = output_path / f'summary_{timestamp}.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜ï¼š{summary_file}")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š å…ƒæ•°æ®é‡‡é›†æ‘˜è¦")
    print("="*60)
    print(f"é¡¹ç›®ï¼š{summary['project']}")
    print(f"é‡‡é›†æ—¶é—´ï¼š{timestamp}")
    print(f"æ€»è¡¨æ•°ï¼š{summary['total_tables']}")
    print(f"æ€»å­—æ®µæ•°ï¼š{summary['total_columns']}")
    print(f"åˆ†åŒºè¡¨æ•°ï¼š{summary['partitioned_tables']}")
    
    if failed_tables:
        print(f"\nâš ï¸  å¤±è´¥çš„è¡¨ ({len(failed_tables)}):")
        for t in failed_tables[:10]:
            print(f"  - {t}")
    
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  - {json_file}")
    print(f"  - {csv_file}")
    print(f"  - {summary_file}")
    
    return all_metadata


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ODPS å…ƒæ•°æ®é‡‡é›†å·¥å…·')
    parser.add_argument('--output', default='odps_metadata', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®
    if not os.getenv('ALIBABA_ACCESSKEY_ID'):
        print("âŒ ç¼ºå°‘ ODPS é…ç½®ç¯å¢ƒå˜é‡")
        print("è¯·è®¾ç½®:")
        print("  export ALIBABA_ACCESSKEY_ID='...'")
        print("  export ALIBABA_ACCESSKEY_SECRET='...'")
        print("  export ALIBABA_ODPS_PROJECT='...'")
        return
    
    # è¿æ¥å¹¶é‡‡é›†
    o = connect_odps()
    crawl_all_metadata(o, args.output)


if __name__ == '__main__':
    main()
