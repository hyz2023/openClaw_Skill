#!/usr/bin/env python3
"""
ODPS å…ƒæ•°æ®é‡‡é›†å·¥å…· (å¢å¼ºç‰ˆ)
- é‡‡é›†è¡¨ç»“æ„ã€å­—æ®µä¿¡æ¯ã€åˆ†åŒºä¿¡æ¯
- è·å–æœ€æ–°æœ‰æ•°æ®çš„åˆ†åŒº
- æ”¯æŒå¢é‡æ›´æ–°
- æ¯ 30 ç§’æ±‡æŠ¥è¿›åº¦

ç¯å¢ƒå˜é‡:
export ALIBABA_ACCESSKEY_ID="your_access_key_id"
export ALIBABA_ACCESSKEY_SECRET="your_access_key_secret"
export ALIBABA_ODPS_PROJECT="superengineproject"
export ALIBABA_ODPS_ENDPOINT="http://service.ap-southeast-1.maxcompute.aliyun.com/api"
"""

import os
import sys
import json
import time
import threading
from datetime import datetime
from pathlib import Path
from odps import ODPS
import pandas as pd


class ProgressReporter:
    """è¿›åº¦æ±‡æŠ¥å™¨ - æ¯ 30 ç§’æ±‡æŠ¥ä¸€æ¬¡"""
    
    def __init__(self, interval: int = 30):
        self.interval = interval
        self.current_table = ""
        self.processed = 0
        self.total = 0
        self.start_time = None
        self.running = False
        self.thread = None
    
    def start(self, total: int):
        """å¯åŠ¨æ±‡æŠ¥çº¿ç¨‹"""
        self.total = total
        self.processed = 0
        self.start_time = datetime.now()
        self.running = True
        self.thread = threading.Thread(target=self._report_loop, daemon=True)
        self.thread.start()
    
    def stop(self):
        """åœæ­¢æ±‡æŠ¥"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=2)
    
    def update(self, table_name: str):
        """æ›´æ–°å½“å‰å¤„ç†çš„è¡¨"""
        self.current_table = table_name
        self.processed += 1
    
    def _report_loop(self):
        """æ±‡æŠ¥å¾ªç¯"""
        while self.running:
            time.sleep(self.interval)
            self._print_progress()
    
    def _print_progress(self):
        """æ‰“å°è¿›åº¦"""
        if self.total == 0:
            return
        
        elapsed = datetime.now() - self.start_time
        elapsed_str = str(elapsed).split('.')[0]
        
        pct = self.processed / self.total * 100
        
        # ä¼°ç®—å‰©ä½™æ—¶é—´
        if self.processed > 0:
            avg_time = elapsed.total_seconds() / self.processed
            remaining = (self.total - self.processed) * avg_time
            remaining_str = f"{remaining/60:.1f}åˆ†é’Ÿ"
        else:
            remaining_str = "æœªçŸ¥"
        
        print(f"\nâ° [{elapsed_str}] è¿›åº¦ï¼š{self.processed}/{self.total} ({pct:.1f}%)")
        print(f"   å½“å‰ï¼š{self.current_table}")
        print(f"   é¢„è®¡å‰©ä½™ï¼š{remaining_str}")
        print(f"   å¤„ç†é€Ÿåº¦ï¼š{self.processed/elapsed.total_seconds()*60:.1f} è¡¨/åˆ†é’Ÿ")
        sys.stdout.flush()


def connect_odps():
    """è¿æ¥ ODPS"""
    access_id = os.getenv('ALIBABA_ACCESSKEY_ID')
    access_key = os.getenv('ALIBABA_ACCESSKEY_SECRET')
    project = os.getenv('ALIBABA_ODPS_PROJECT', 'superengineproject')
    endpoint = os.getenv('ALIBABA_ODPS_ENDPOINT', 'http://service.ap-southeast-1.maxcompute.aliyun.com/api')
    
    print(f"è¿æ¥ ODPS é¡¹ç›®ï¼š{project}")
    
    return ODPS(
        access_id=access_id,
        secret_access_key=access_key,
        project=project,
        endpoint=endpoint
    )


def list_all_tables(o: ODPS) -> list:
    """åˆ—å‡ºæ‰€æœ‰è¡¨"""
    print("\nğŸ“‹ è·å–è¡¨åˆ—è¡¨...")
    tables = []
    
    for table in o.list_tables():
        tables.append(table.name)
    
    print(f"æ‰¾åˆ° {len(tables)} å¼ è¡¨")
    return tables


def get_partition_data_status(o: ODPS, table_name: str) -> dict:
    """
    è·å–è¡¨çš„åˆ†åŒºæ•°æ®çŠ¶æ€
    è¿”å›æœ€æ–°æœ‰æ•°æ®çš„åˆ†åŒºä¿¡æ¯
    """
    try:
        table = o.get_table(table_name)
        
        if not table.table_schema.partitions:
            return {
                'is_partitioned': False,
                'latest_partition': None,
                'partition_count': 0,
                'has_data': None
            }
        
        # è·å–æ‰€æœ‰åˆ†åŒº
        partitions = list(table.partitions)
        
        if not partitions:
            return {
                'is_partitioned': True,
                'latest_partition': None,
                'partition_count': 0,
                'has_data': False
            }
        
        # æŒ‰åˆ†åŒºå€¼æ’åº (å‡è®¾ pt æ ¼å¼ä¸º yyyymmdd)
        partition_info = []
        for pt in partitions:
            pt_name = pt.name
            # æå–åˆ†åŒºå€¼ (å¦‚ pt='20260226')
            if '=' in pt_name:
                pt_value = pt_name.split('=')[1].strip("'\"")
            else:
                pt_value = pt_name
            
            partition_info.append({
                'name': pt_name,
                'value': pt_value,
                'size': pt.size if hasattr(pt, 'size') else 0,
                'records': pt.records if hasattr(pt, 'records') else 0
            })
        
        # æŒ‰åˆ†åŒºå€¼æ’åº
        partition_info.sort(key=lambda x: x['value'], reverse=True)
        
        # æ‰¾åˆ°æœ€æ–°æœ‰æ•°æ®çš„åˆ†åŒº
        latest_with_data = None
        for pt in partition_info:
            if pt['records'] > 0 or pt['size'] > 0:
                latest_with_data = pt
                break
        
        return {
            'is_partitioned': True,
            'latest_partition': latest_with_data,
            'all_partitions': partition_info[:10],  # åªä¿ç•™å‰ 10 ä¸ª
            'partition_count': len(partitions),
            'has_data': latest_with_data is not None
        }
        
    except Exception as e:
        return {
            'is_partitioned': True,
            'latest_partition': None,
            'partition_count': 0,
            'has_data': None,
            'error': str(e)
        }


def get_table_metadata(o: ODPS, table_name: str, check_partitions: bool = True) -> dict:
    """è·å–å•å¼ è¡¨çš„è¯¦ç»†å…ƒæ•°æ®"""
    try:
        table = o.get_table(table_name)
        
        # åŸºæœ¬ä¿¡æ¯
        metadata = {
            'table_name': table.name,
            'comment': getattr(table, 'comment', ''),
            'create_time': str(table.creation_time) if hasattr(table, 'creation_time') else None,
            'last_modified_time': str(table.last_data_modified_time) if hasattr(table, 'last_data_modified_time') else None,
            'size': table.size if hasattr(table, 'size') else None,
            'is_virtual_view': getattr(table, 'is_virtual_view', False),
            'lifecycle': getattr(table, 'lifecycle', None)
        }
        
        # å­—æ®µä¿¡æ¯
        columns = []
        for col in table.table_schema.columns:
            col_info = {
                'name': col.name,
                'type': str(col.type),
                'comment': getattr(col, 'comment', ''),
                'is_nullable': getattr(col, 'is_nullable', True)
            }
            columns.append(col_info)
        
        metadata['columns'] = columns
        metadata['column_count'] = len(columns)
        
        # åˆ†åŒºä¿¡æ¯ (å¯é€‰ï¼Œè€—æ—¶)
        if check_partitions:
            partition_status = get_partition_data_status(o, table_name)
            metadata['partition_status'] = partition_status
        else:
            metadata['partition_status'] = {
                'is_partitioned': False,
                'latest_partition': None
            }
        
        return metadata
        
    except Exception as e:
        print(f"âš ï¸  è·å–è¡¨ {table_name} å…ƒæ•°æ®å¤±è´¥ï¼š{e}")
        return None


def load_existing_metadata(output_dir: str) -> dict:
    """åŠ è½½å·²æœ‰çš„å…ƒæ•°æ® (ç”¨äºå¢é‡æ›´æ–°)"""
    latest_json = Path(output_dir) / 'metadata_latest.json'
    
    if not latest_json.exists():
        return None
    
    try:
        with open(latest_json, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"âœ… åŠ è½½å·²æœ‰å…ƒæ•°æ®ï¼š{data.get('table_count', 0)} å¼ è¡¨")
            return data
    except:
        return None


def crawl_all_metadata(o: ODPS, output_dir: str = 'odps_metadata', incremental: bool = True):
    """
    é‡‡é›†æ‰€æœ‰è¡¨çš„å…ƒæ•°æ®
    
    Args:
        o: ODPS è¿æ¥
        output_dir: è¾“å‡ºç›®å½•
        incremental: æ˜¯å¦å¢é‡æ›´æ–°
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è·å–æ‰€æœ‰è¡¨
    table_names = list_all_tables(o)
    
    # åŠ è½½å·²æœ‰å…ƒæ•°æ® (å¢é‡æ¨¡å¼)
    existing_data = None
    if incremental:
        existing_data = load_existing_metadata(output_dir)
    
    all_metadata = []
    failed_tables = []
    updated_tables = 0
    unchanged_tables = 0
    
    # æ„å»ºå·²æœ‰å…ƒæ•°æ®çš„ç´¢å¼•
    existing_index = {}
    if existing_data and 'tables' in existing_data:
        for table in existing_data['tables']:
            existing_index[table['table_name']] = table
    
    # åˆå§‹åŒ–è¿›åº¦æ±‡æŠ¥
    reporter = ProgressReporter(interval=30)
    reporter.start(len(table_names))
    
    start_time = datetime.now()
    
    # é€è¡¨é‡‡é›†
    for i, table_name in enumerate(table_names, 1):
        reporter.update(table_name)
        
        # å¢é‡æ£€æŸ¥
        if incremental and table_name in existing_index:
            existing_table = existing_index[table_name]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–° (è¡¨å¤§å°å˜åŒ–æˆ–æœ€åä¿®æ”¹æ—¶é—´å˜åŒ–)
            # ç®€å•ç­–ç•¥ï¼šæ¯æ¬¡éƒ½æ›´æ–°åˆ†åŒºä¿¡æ¯ï¼Œå…¶ä»–ä¿¡æ¯å¦‚æœè¡¨å¤§å°æ²¡å˜å°±è·³è¿‡
            try:
                current_table = o.get_table(table_name)
                current_size = current_table.size if hasattr(current_table, 'size') else 0
                existing_size = existing_table.get('size', 0)
                
                # å¦‚æœè¡¨å¤§å°æ²¡å˜ï¼Œä¸”å·²æœ‰åˆ†åŒºä¿¡æ¯ï¼Œè·³è¿‡
                if current_size == existing_size and existing_table.get('partition_status', {}).get('latest_partition'):
                    all_metadata.append(existing_table)
                    unchanged_tables += 1
                    continue
                else:
                    # éœ€è¦æ›´æ–°
                    print(f"  ğŸ“ è¡¨æœ‰æ›´æ–°ï¼Œé‡æ–°é‡‡é›†ï¼š{table_name}")
            except:
                pass
        
        # é‡‡é›†å…ƒæ•°æ® (æ£€æŸ¥åˆ†åŒº)
        metadata = get_table_metadata(o, table_name, check_partitions=True)
        
        if metadata:
            all_metadata.append(metadata)
            updated_tables += 1
            
            # æ¯é‡‡é›† 50 å¼ è¡¨ä¿å­˜ä¸€æ¬¡è¿›åº¦
            if i % 50 == 0:
                save_incremental_progress(output_path, all_metadata, failed_tables, i, len(table_names))
        else:
            failed_tables.append(table_name)
    
    # åœæ­¢è¿›åº¦æ±‡æŠ¥
    reporter.stop()
    
    # æœ€ç»ˆç»Ÿè®¡
    elapsed = datetime.now() - start_time
    elapsed_str = str(elapsed).split('.')[0]
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # 1. JSON æ ¼å¼ (å®Œæ•´å…ƒæ•°æ®)
    json_file = output_path / f'metadata_{timestamp}.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            'project': os.getenv('ALIBABA_ODPS_PROJECT'),
            'crawl_time': timestamp,
            'crawl_duration': elapsed_str,
            'table_count': len(all_metadata),
            'updated_tables': updated_tables,
            'unchanged_tables': unchanged_tables,
            'failed_tables': failed_tables,
            'tables': all_metadata
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å®Œæ•´å…ƒæ•°æ®å·²ä¿å­˜ï¼š{json_file}")
    
    # 2. CSV æ ¼å¼ (å­—æ®µæ¸…å•)
    all_columns = []
    for table in all_metadata:
        for col in table['columns']:
            all_columns.append({
                'table_name': table['table_name'],
                'column_name': col['name'],
                'column_type': col['type'],
                'comment': col['comment'],
                'is_nullable': col['is_nullable']
            })
    
    csv_file = output_path / f'columns_{timestamp}.csv'
    df = pd.DataFrame(all_columns)
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    
    print(f"âœ… å­—æ®µæ¸…å•å·²ä¿å­˜ï¼š{csv_file}")
    
    # 3. åˆ›å»ºæœ€æ–°å…ƒæ•°æ®çš„é“¾æ¥
    latest_json = output_path / 'metadata_latest.json'
    latest_csv = output_path / 'columns_latest.csv'
    
    if latest_json.exists():
        latest_json.unlink()
    if latest_csv.exists():
        latest_csv.unlink()
    
    import shutil
    shutil.copy(json_file, latest_json)
    shutil.copy(csv_file, latest_csv)
    
    print(f"âœ… æœ€æ–°å…ƒæ•°æ®é“¾æ¥å·²åˆ›å»º")
    
    # 4. ç»Ÿè®¡æ‘˜è¦
    partitioned_tables = sum(1 for t in all_metadata if t.get('partition_status', {}).get('is_partitioned'))
    tables_with_data = sum(1 for t in all_metadata if t.get('partition_status', {}).get('has_data'))
    
    summary = {
        'project': os.getenv('ALIBABA_ODPS_PROJECT'),
        'crawl_time': timestamp,
        'crawl_duration': elapsed_str,
        'total_tables': len(all_metadata),
        'total_columns': sum(t['column_count'] for t in all_metadata),
        'partitioned_tables': partitioned_tables,
        'tables_with_data': tables_with_data,
        'updated_tables': updated_tables,
        'unchanged_tables': unchanged_tables,
        'failed_tables': failed_tables,
        'top_tables_by_columns': sorted(
            [(t['table_name'], t['column_count']) for t in all_metadata],
            key=lambda x: x[1],
            reverse=True
        )[:10],
        'tables_with_latest_partition': [
            {
                'table': t['table_name'],
                'latest_partition': t['partition_status'].get('latest_partition', {}).get('value'),
                'has_data': t['partition_status'].get('has_data')
            }
            for t in all_metadata[:20]  # åªæ˜¾ç¤ºå‰ 20 ä¸ª
        ]
    }
    
    summary_file = output_path / f'summary_{timestamp}.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜ï¼š{summary_file}")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*70)
    print("ğŸ“Š å…ƒæ•°æ®é‡‡é›†å®Œæˆ")
    print("="*70)
    print(f"é¡¹ç›®ï¼š{summary['project']}")
    print(f"é‡‡é›†æ—¶é—´ï¼š{timestamp}")
    print(f"è€—æ—¶ï¼š{elapsed_str}")
    print(f"æ€»è¡¨æ•°ï¼š{summary['total_tables']}")
    print(f"æ€»å­—æ®µæ•°ï¼š{summary['total_columns']}")
    print(f"åˆ†åŒºè¡¨æ•°ï¼š{partitioned_tables}")
    print(f"æœ‰æ•°æ®çš„è¡¨ï¼š{tables_with_data}")
    print(f"æ›´æ–°è¡¨æ•°ï¼š{updated_tables}")
    print(f"è·³è¿‡è¡¨æ•°ï¼š{unchanged_tables}")
    
    if failed_tables:
        print(f"\nâš ï¸  å¤±è´¥çš„è¡¨ ({len(failed_tables)}):")
        for t in failed_tables[:10]:
            print(f"  - {t}")
    
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  - {json_file}")
    print(f"  - {csv_file}")
    print(f"  - {summary_file}")
    
    return all_metadata


def save_incremental_progress(output_path: Path, all_metadata: list, failed_tables: list, 
                              current: int, total: int):
    """ä¿å­˜å¢é‡è¿›åº¦"""
    temp_file = output_path / 'metadata_progress.json'
    
    with open(temp_file, 'w', encoding='utf-8') as f:
        json.dump({
            'progress': f"{current}/{total}",
            'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
            'tables': all_metadata,
            'failed_tables': failed_tables
        }, f, ensure_ascii=False, indent=2)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ODPS å…ƒæ•°æ®é‡‡é›†å·¥å…·')
    parser.add_argument('--output', default='odps_metadata', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--full', action='store_true', help='å…¨é‡é‡‡é›† (ä¸å¢é‡)')
    parser.add_argument('--no-partition-check', action='store_true', help='è·³è¿‡åˆ†åŒºæ£€æŸ¥')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®
    if not os.getenv('ALIBABA_ACCESSKEY_ID'):
        print("âŒ ç¼ºå°‘ ODPS é…ç½®ç¯å¢ƒå˜é‡")
        print("è¯·è®¾ç½®:")
        print("  export ALIBABA_ACCESSKEY_ID='...'")
        print("  export ALIBABA_ACCESSKEY_SECRET='...'")
        print("  export ALIBABA_ODPS_PROJECT='...'")
        return
    
    # è¿æ¥å¹¶é‡‡é›†
    o = connect_odps()
    crawl_all_metadata(
        o, 
        args.output, 
        incremental=not args.full
    )


if __name__ == '__main__':
    main()
