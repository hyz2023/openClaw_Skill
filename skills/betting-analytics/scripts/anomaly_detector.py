#!/usr/bin/env python3
"""
åšå½©æ•°æ®å¼‚å¸¸ç‚¹æ£€æµ‹å·¥å…·
Anomaly Detector for Betting Data

åŠŸèƒ½:
- Z-Score å¼‚å¸¸æ£€æµ‹
- IQR ç¦»ç¾¤å€¼æ£€æµ‹
- Isolation Forest æœºå™¨å­¦ä¹ æ£€æµ‹
- å¤šæ–¹æ³•é›†æˆè¯„åˆ†
- å¼‚å¸¸æŠ¥å‘Šç”Ÿæˆ
"""

import argparse
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from pathlib import Path


def load_data(data_path: str) -> pd.DataFrame:
    """åŠ è½½æ•°æ®æ–‡ä»¶"""
    path = Path(data_path)
    if path.suffix == '.csv':
        df = pd.read_csv(data_path)
    elif path.suffix == '.json':
        df = pd.read_json(data_path)
    else:
        raise ValueError(f"Unsupported file format: {path.suffix}")
    
    return df


def detect_zscore_anomalies(series: pd.Series, threshold: float = 3.0) -> pd.Series:
    """
    Z-Score å¼‚å¸¸æ£€æµ‹
    
    Args:
        series: æ•°æ®åºåˆ—
        threshold: Z-Score é˜ˆå€¼ (é»˜è®¤ 3.0)
    
    Returns:
        å¼‚å¸¸è¯„åˆ†åºåˆ— (0-1)
    """
    mean = series.mean()
    std = series.std()
    
    if std == 0:
        return pd.Series(0, index=series.index)
    
    z_scores = np.abs((series - mean) / std)
    # å½’ä¸€åŒ–åˆ° 0-1
    anomaly_scores = np.minimum(z_scores / threshold, 1.0)
    
    return anomaly_scores


def detect_iqr_anomalies(series: pd.Series, k: float = 1.5) -> pd.Series:
    """
    IQR (å››åˆ†ä½è·) å¼‚å¸¸æ£€æµ‹
    
    Args:
        series: æ•°æ®åºåˆ—
        k: IQR å€æ•° (é»˜è®¤ 1.5)
    
    Returns:
        å¼‚å¸¸è¯„åˆ†åºåˆ— (0-1)
    """
    q1 = series.quantile(0.25)
    q3 = series.quantile(0.75)
    iqr = q3 - q1
    
    lower_bound = q1 - k * iqr
    upper_bound = q3 + k * iqr
    
    # è®¡ç®—è·ç¦»è¾¹ç•Œçš„è·ç¦»
    distances = np.maximum(lower_bound - series, series - upper_bound, 0)
    
    # å½’ä¸€åŒ–
    range_val = upper_bound - lower_bound
    if range_val > 0:
        anomaly_scores = np.minimum(distances / (k * iqr), 1.0)
    else:
        anomaly_scores = pd.Series(0, index=series.index)
    
    return anomaly_scores


def detect_isolation_forest_anomalies(df: pd.DataFrame, 
                                       contamination: float = 0.1,
                                       n_estimators: int = 100) -> pd.Series:
    """
    Isolation Forest å¼‚å¸¸æ£€æµ‹
    
    Args:
        df: æ•°æ® DataFrame (æ•°å€¼åˆ—)
        contamination: é¢„æœŸå¼‚å¸¸æ¯”ä¾‹
        n_estimators: æ ‘çš„æ•°é‡
    
    Returns:
        å¼‚å¸¸è¯„åˆ†åºåˆ— (0-1)
    """
    # é€‰æ‹©æ•°å€¼åˆ—
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) == 0:
        raise ValueError("No numeric columns found in data")
    
    X = df[numeric_cols].dropna()
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Isolation Forest
    model = IsolationForest(
        n_estimators=n_estimators,
        contamination=contamination,
        random_state=42,
        n_jobs=-1
    )
    
    # è·å–å¼‚å¸¸è¯„åˆ† (è´Ÿå€¼è¡¨ç¤ºå¼‚å¸¸)
    scores = model.decision_function(X_scaled)
    
    # è½¬æ¢ä¸º 0-1 è¯„åˆ† (1 è¡¨ç¤ºæœ€å¼‚å¸¸)
    min_score = scores.min()
    max_score = scores.max()
    
    if max_score - min_score > 0:
        anomaly_scores = 1 - (scores - min_score) / (max_score - min_score)
    else:
        anomaly_scores = np.zeros(len(scores))
    
    # è¿”å›ä¸åŸ DataFrame å¯¹é½çš„ Series
    result = pd.Series(0.0, index=df.index)
    result.loc[X.index] = anomaly_scores
    
    return result


def detect_time_series_anomalies(series: pd.Series, window: int = 20) -> pd.Series:
    """
    æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ (åŸºäºæ»šåŠ¨ç»Ÿè®¡)
    
    Args:
        series: æ—¶é—´åºåˆ—æ•°æ®
        window: æ»šåŠ¨çª—å£å¤§å°
    
    Returns:
        å¼‚å¸¸è¯„åˆ†åºåˆ— (0-1)
    """
    # è®¡ç®—æ»šåŠ¨å‡å€¼å’Œæ ‡å‡†å·®
    rolling_mean = series.rolling(window=window, center=True, min_periods=1).mean()
    rolling_std = series.rolling(window=window, center=True, min_periods=1).std()
    
    # è®¡ç®—æ®‹å·®
    residuals = series - rolling_mean
    
    # å¤„ç†æ ‡å‡†å·®ä¸º 0 çš„æƒ…å†µ
    rolling_std = rolling_std.replace(0, np.nan).fillna(1e-6)
    
    # è®¡ç®—æ ‡å‡†åŒ–æ®‹å·®
    z_scores = np.abs(residuals / rolling_std)
    
    # å½’ä¸€åŒ–åˆ° 0-1
    threshold = 3.0
    anomaly_scores = np.minimum(z_scores / threshold, 1.0)
    
    return anomaly_scores.fillna(0)


def ensemble_anomaly_detection(df: pd.DataFrame, 
                                methods: list = None,
                                weights: dict = None) -> pd.Series:
    """
    é›†æˆå¼‚å¸¸æ£€æµ‹ (å¤šæ–¹æ³•åŠ æƒ)
    
    Args:
        df: æ•°æ® DataFrame
        methods: ä½¿ç”¨çš„æ–¹æ³•åˆ—è¡¨
        weights: å„æ–¹æ³•æƒé‡
    
    Returns:
        ç»¼åˆå¼‚å¸¸è¯„åˆ†åºåˆ— (0-1)
    """
    if methods is None:
        methods = ['zscore', 'iqr', 'isolation_forest']
    
    if weights is None:
        weights = {
            'zscore': 0.3,
            'iqr': 0.3,
            'isolation_forest': 0.4
        }
    
    scores = {}
    
    # é€‰æ‹©æ•°å€¼åˆ—è¿›è¡Œåˆ†æ
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        series = df[col].dropna()
        if len(series) < 10:
            continue
        
        if 'zscore' in methods:
            zscore_scores = detect_zscore_anomalies(series)
            if col not in scores:
                scores[col] = {}
            scores[col]['zscore'] = zscore_scores
        
        if 'iqr' in methods:
            iqr_scores = detect_iqr_anomalies(series)
            if col not in scores:
                scores[col] = {}
            scores[col]['iqr'] = iqr_scores
    
    # Isolation Forest ä½¿ç”¨æ‰€æœ‰æ•°å€¼åˆ—
    if 'isolation_forest' in methods and len(numeric_cols) > 0:
        try:
            if_scores = detect_isolation_forest_anomalies(df[numeric_cols])
            scores['global'] = {'isolation_forest': if_scores}
        except Exception as e:
            print(f"Isolation Forest æ£€æµ‹å¤±è´¥ï¼š{e}")
    
    # æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ (å¦‚æœæœ‰æ—¶é—´åˆ—)
    time_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]
    if time_cols and len(numeric_cols) > 0:
        df_sorted = df.sort_values(time_cols[0])
        for col in numeric_cols[:3]:  # åªå¯¹å‰ 3 ä¸ªæ•°å€¼åˆ—åšæ—¶é—´åºåˆ—åˆ†æ
            series = df_sorted[col].dropna()
            if len(series) >= 10:
                ts_scores = detect_time_series_anomalies(series)
                if col not in scores:
                    scores[col] = {}
                scores[col]['time_series'] = ts_scores.values
    
    # ç»¼åˆæ‰€æœ‰è¯„åˆ†
    all_scores = []
    all_weights = []
    
    for col, method_scores in scores.items():
        for method, score_series in method_scores.items():
            if isinstance(score_series, np.ndarray):
                score_series = pd.Series(score_series, index=df.index[:len(score_series)])
            
            # ç¡®ä¿ç´¢å¼•å¯¹é½
            score_series = score_series.reindex(df.index, fill_value=0)
            all_scores.append(score_series)
            
            weight = weights.get(method, 1.0 / len(methods))
            all_weights.append(weight)
    
    if not all_scores:
        return pd.Series(0.0, index=df.index)
    
    # åŠ æƒå¹³å‡
    weighted_scores = sum(s * w for s, w in zip(all_scores, all_weights))
    total_weight = sum(all_weights)
    
    if total_weight > 0:
        final_scores = weighted_scores / total_weight
    else:
        final_scores = weighted_scores
    
    return final_scores.clip(0, 1)


def analyze_anomalies(df: pd.DataFrame, 
                      method: str = 'ensemble',
                      threshold: float = 0.7) -> dict:
    """
    æ‰§è¡Œå¼‚å¸¸åˆ†æ
    
    Args:
        df: æ•°æ® DataFrame
        method: æ£€æµ‹æ–¹æ³• (zscore/iqr/isolation_forest/ensemble)
        threshold: å¼‚å¸¸é˜ˆå€¼
    
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    # é€‰æ‹©æ•°å€¼åˆ—
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    if len(numeric_cols) == 0:
        raise ValueError("No numeric columns found in data")
    
    # æ‰§è¡Œæ£€æµ‹
    if method == 'zscore':
        # å¯¹ä¸»è¦æ•°å€¼åˆ—è¿›è¡Œ Z-Score æ£€æµ‹
        primary_col = numeric_cols[0]
        anomaly_scores = detect_zscore_anomalies(df[primary_col])
    elif method == 'iqr':
        primary_col = numeric_cols[0]
        anomaly_scores = detect_iqr_anomalies(df[primary_col])
    elif method == 'isolation_forest':
        anomaly_scores = detect_isolation_forest_anomalies(df[numeric_cols])
    else:  # ensemble
        anomaly_scores = ensemble_anomaly_detection(df)
    
    # è¯†åˆ«å¼‚å¸¸ç‚¹
    anomalies = df[anomaly_scores >= threshold].copy()
    anomalies['anomaly_score'] = anomaly_scores[anomaly_scores >= threshold]
    
    # ç»Ÿè®¡ä¿¡æ¯
    results = {
        'total_records': len(df),
        'anomaly_count': len(anomalies),
        'anomaly_rate': len(anomalies) / len(df) if len(df) > 0 else 0,
        'method': method,
        'threshold': threshold,
        'score_stats': {
            'mean': float(anomaly_scores.mean()),
            'std': float(anomaly_scores.std()),
            'max': float(anomaly_scores.max()),
            'min': float(anomaly_scores.min()),
            'median': float(anomaly_scores.median())
        },
        'anomalies': anomalies,
        'scores': anomaly_scores
    }
    
    return results


def generate_report(results: dict, output_path: str = None) -> str:
    """ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š"""
    report = []
    report.append("## ğŸ” å¼‚å¸¸ç‚¹æ£€æµ‹æŠ¥å‘Š")
    report.append("")
    report.append(f"**æ£€æµ‹æ–¹æ³•**: {results['method']}")
    report.append(f"**å¼‚å¸¸é˜ˆå€¼**: {results['threshold']}")
    report.append("")
    
    report.append("### æ£€æµ‹æ‘˜è¦")
    report.append(f"| æŒ‡æ ‡ | æ•°å€¼ |")
    report.append(f"|------|------|")
    report.append(f"| æ€»è®°å½•æ•° | {results['total_records']} |")
    report.append(f"| å¼‚å¸¸ç‚¹æ•° | {results['anomaly_count']} |")
    report.append(f"| å¼‚å¸¸ç‡ | {results['anomaly_rate']:.2%} |")
    report.append("")
    
    report.append("### è¯„åˆ†ç»Ÿè®¡")
    stats = results['score_stats']
    report.append(f"| ç»Ÿè®¡é¡¹ | æ•°å€¼ |")
    report.append(f"|--------|------|")
    report.append(f"| å¹³å‡å€¼ | {stats['mean']:.4f} |")
    report.append(f"| æ ‡å‡†å·® | {stats['std']:.4f} |")
    report.append(f"| æœ€å¤§å€¼ | {stats['max']:.4f} |")
    report.append(f"| ä¸­ä½æ•° | {stats['median']:.4f} |")
    report.append("")
    
    anomalies = results['anomalies']
    if len(anomalies) > 0:
        report.append("### å¼‚å¸¸ç‚¹è¯¦æƒ…")
        report.append("")
        
        # æ˜¾ç¤ºå‰ 10 ä¸ªå¼‚å¸¸ç‚¹
        top_anomalies = anomalies.nlargest(10, 'anomaly_score')
        
        for idx, row in top_anomalies.iterrows():
            report.append(f"**è®°å½• #{idx}** (å¼‚å¸¸è¯„åˆ†ï¼š{row['anomaly_score']:.3f})")
            # æ˜¾ç¤ºå…³é”®ä¿¡æ¯
            key_cols = [col for col in row.index if col != 'anomaly_score'][:5]
            for col in key_cols:
                report.append(f"- {col}: {row[col]}")
            report.append("")
    else:
        report.append("### å¼‚å¸¸ç‚¹è¯¦æƒ…")
        report.append("")
        report.append("æœªå‘ç°å¼‚å¸¸è®°å½• âœ…")
        report.append("")
    
    report.append("### å»ºè®®")
    anomaly_rate = results['anomaly_rate']
    if anomaly_rate > 0.1:
        report.append("âš ï¸ å¼‚å¸¸ç‡è¾ƒé«˜ (>10%)ï¼Œå»ºè®®:")
        report.append("1. æ£€æŸ¥æ•°æ®è´¨é‡å’Œé‡‡é›†æµç¨‹")
        report.append("2. è°ƒæŸ¥å¼‚å¸¸ç‚¹èƒŒåçš„åŸå› ")
        report.append("3. è€ƒè™‘è°ƒæ•´æ£€æµ‹é˜ˆå€¼")
    elif anomaly_rate > 0.05:
        report.append("âš¡ å¼‚å¸¸ç‡ä¸­ç­‰ (5-10%)ï¼Œå»ºè®®:")
        report.append("1. é‡ç‚¹å…³æ³¨é«˜è¯„åˆ†å¼‚å¸¸ç‚¹")
        report.append("2. åˆ†æå¼‚å¸¸ç‚¹çš„æ—¶é—´åˆ†å¸ƒ")
    else:
        report.append("âœ… å¼‚å¸¸ç‡æ­£å¸¸ (<5%)ï¼Œæ•°æ®è´¨é‡è‰¯å¥½")
    
    report_text = '\n'.join(report)
    
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
    
    return report_text


def main():
    parser = argparse.ArgumentParser(description='åšå½©æ•°æ®å¼‚å¸¸ç‚¹æ£€æµ‹å·¥å…·')
    parser.add_argument('--data', required=True, help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ (CSV/JSON)')
    parser.add_argument('--method', default='ensemble',
                        choices=['zscore', 'iqr', 'isolation_forest', 'ensemble'],
                        help='æ£€æµ‹æ–¹æ³•')
    parser.add_argument('--threshold', type=float, default=0.7, 
                        help='å¼‚å¸¸é˜ˆå€¼ (0-1)')
    parser.add_argument('--output', default=None, help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--save-scores', default=None, 
                        help='ä¿å­˜å¼‚å¸¸è¯„åˆ†åˆ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®
    print(f"åŠ è½½æ•°æ®ï¼š{args.data}")
    df = load_data(args.data)
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
    
    # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
    print(f"ä½¿ç”¨ {args.method} æ–¹æ³•æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")
    results = analyze_anomalies(df, args.method, args.threshold)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(results, args.output)
    print("\n" + report)
    
    # ä¿å­˜è¯„åˆ†
    if args.save_scores:
        scores_df = pd.DataFrame({
            'anomaly_score': results['scores']
        })
        scores_df.to_csv(args.save_scores, index=True)
        print(f"\nå¼‚å¸¸è¯„åˆ†å·²ä¿å­˜è‡³ï¼š{args.save_scores}")
    
    if args.output:
        print(f"\næŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{args.output}")


if __name__ == '__main__':
    main()
