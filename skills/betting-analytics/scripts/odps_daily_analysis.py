#!/usr/bin/env python3
"""
ODPS æŠ•æ³¨æ•°æ®æ—¥å¸¸åˆ†æè„šæœ¬
ç»Ÿè®¡æœ€è¿‘ä¸€ä¸ªæœˆæ¯å¤©çš„æŠ•æ³¨æ•°æ®ï¼Œè¿›è¡Œè¶‹åŠ¿åˆ†æå’Œå¼‚å¸¸æ£€æµ‹

ä½¿ç”¨å‰é…ç½®ç¯å¢ƒå˜é‡:
export ALIBABA_ACCESSKEY_ID="your_access_key_id"
export ALIBABA_ACCESSKEY_SECRET="your_access_key_secret"
export ALIBABA_ODPS_ENDPOINT="http://service.odps.aliyun.com/api"
export ALIBABA_ODPS_PROJECT="your_project_name"
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import sys

# å¼•å…¥è¶‹åŠ¿åˆ†æå’Œå¼‚å¸¸æ£€æµ‹æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from trend_analyzer import analyze_trend, generate_report as trend_report
from anomaly_detector import analyze_anomalies, generate_report as anomaly_report
from drill_down_analyzer import analyze_drill_down, generate_report as drilldown_report


def check_odps_config():
    """æ£€æŸ¥ ODPS é…ç½®"""
    required_vars = [
        'ALIBABA_ACCESSKEY_ID',
        'ALIBABA_ACCESSKEY_SECRET',
        'ALIBABA_ODPS_PROJECT'
    ]
    
    missing = [var for var in required_vars if not os.getenv(var)]
    
    if missing:
        print("âŒ ç¼ºå°‘ ODPS é…ç½®ç¯å¢ƒå˜é‡:")
        for var in missing:
            print(f"   - {var}")
        print("\nè¯·è®¾ç½®ç¯å¢ƒå˜é‡åé‡è¯•")
        return False
    
    return True


def query_odps_daily_data(project: str, days: int = 30) -> pd.DataFrame:
    """
    ä» ODPS æŸ¥è¯¢æœ€è¿‘ N å¤©æ¯å¤©çš„æŠ•æ³¨æ•°æ®
    
    Args:
        project: ODPS é¡¹ç›®åç§°
        days: æŸ¥è¯¢å¤©æ•° (é»˜è®¤ 30 å¤©)
    
    Returns:
        DataFrame with columns: dt, user_count, bet_count, total_amount
    """
    try:
        from odps import ODPS
    except ImportError:
        print("è¯·å®‰è£… ODPS åº“ï¼špip install odps")
        return None
    
    # åˆå§‹åŒ– ODPS å®¢æˆ·ç«¯
    access_id = os.getenv('ALIBABA_ACCESSKEY_ID')
    access_key = os.getenv('ALIBABA_ACCESSKEY_SECRET')
    endpoint = os.getenv('ALIBABA_ODPS_ENDPOINT', 'http://service.odps.aliyun.com/api')
    
    print(f"è¿æ¥ ODPS é¡¹ç›®ï¼š{project}")
    
    o = ODPS(
        access_id=access_id,
        secret_access_key=access_key,
        project=project,
        endpoint=endpoint
    )
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    end_date = datetime.now().strftime('%Y%m%d')
    start_date = (datetime.now() - timedelta(days=days)).strftime('%Y%m%d')
    
    # SQL æŸ¥è¯¢ï¼šç»Ÿè®¡æ¯å¤©çš„æŠ•æ³¨æ•°æ®
    sql = f"""
    SELECT 
        dt AS date,
        COUNT(DISTINCT login_name) AS user_count,
        COUNT(*) AS bet_count,
        SUM(bet_amount) AS total_amount,
        AVG(bet_amount) AS avg_amount,
        COUNT(DISTINCT ordersourcetype) AS source_count
    FROM 
        t_order_all
    WHERE 
        dt >= '{start_date}'
        AND dt <= '{end_date}'
        AND login_name IS NOT NULL
    GROUP BY 
        dt
    ORDER BY 
        dt ASC
    """
    
    print(f"æ‰§è¡Œ SQL æŸ¥è¯¢ (æœ€è¿‘ {days} å¤©)...")
    
    # æ‰§è¡ŒæŸ¥è¯¢
    with o.execute_sql(sql).open_reader() as reader:
        df = reader.to_pandas()
    
    # è½¬æ¢æ—¥æœŸåˆ—
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'].astype(str))
        df = df.set_index('date')
    
    print(f"æŸ¥è¯¢å®Œæˆï¼Œè·å– {len(df)} å¤©çš„æ•°æ®")
    
    return df


def generate_sample_daily_data(days: int = 30) -> pd.DataFrame:
    """ç”Ÿæˆç¤ºä¾‹æ—¥å¸¸æ•°æ® (ç”¨äºæµ‹è¯•)"""
    np.random.seed(42)
    
    # ç”Ÿæˆæ—¥æœŸåºåˆ—
    dates = pd.date_range(end=datetime.now(), periods=days, freq='D')
    
    # ç”ŸæˆåŸºç¡€æ•°æ® (å¸¦æœ‰è¶‹åŠ¿å’Œå‘¨æœŸæ€§)
    trend = np.linspace(10000, 12000, days)  # ä¸Šå‡è¶‹åŠ¿
    weekly_pattern = np.sin(np.arange(days) * 2 * np.pi / 7) * 2000  # å‘¨å‘¨æœŸ
    noise = np.random.normal(0, 1500, days)  # éšæœºå™ªå£°
    
    user_count = (trend + weekly_pattern + noise).astype(int)
    user_count = np.maximum(user_count, 1000)  # ç¡®ä¿éè´Ÿ
    
    # æŠ•æ³¨æ¬¡æ•° (ä¸ç”¨æˆ·æ•°ç›¸å…³ï¼Œä½†æœ‰é¢å¤–æ³¢åŠ¨)
    bet_count = (user_count * np.random.uniform(3, 6, days)).astype(int)
    
    # æŠ•æ³¨é‡‘é¢
    total_amount = user_count * np.random.uniform(100, 300, days)
    
    # å¹³å‡æŠ•æ³¨é¢
    avg_amount = total_amount / bet_count
    
    # æ·»åŠ å‡ ä¸ªå¼‚å¸¸ç‚¹
    anomaly_days = np.random.choice(days, 3)
    for day in anomaly_days:
        user_count[day] *= np.random.choice([0.3, 2.5])  # å¼‚å¸¸ä½æˆ–å¼‚å¸¸é«˜
        bet_count[day] = int(bet_count[day] * np.random.choice([0.4, 2.0]))
        total_amount[day] *= np.random.choice([0.5, 1.8])
    
    df = pd.DataFrame({
        'user_count': user_count,
        'bet_count': bet_count,
        'total_amount': total_amount,
        'avg_amount': avg_amount,
        'source_count': np.random.randint(5, 10, days)
    }, index=dates)
    
    return df


def daily_trend_analysis(df: pd.DataFrame, output_dir: str = None) -> dict:
    """
    æ—¥å¸¸æ•°æ®è¶‹åŠ¿åˆ†æ
    """
    print("\n" + "="*60)
    print("ğŸ“ˆ æ—¥å¸¸è¶‹åŠ¿åˆ†æ")
    print("="*60)
    
    results = {}
    
    for metric in ['user_count', 'bet_count', 'total_amount']:
        if metric not in df.columns:
            continue
        
        print(f"\n## {metric} è¶‹åŠ¿åˆ†æ")
        print("-" * 40)
        
        series = df[metric]
        
        # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
        ma7 = series.rolling(7).mean()
        ma14 = series.rolling(14).mean()
        
        # è¶‹åŠ¿æ–¹å‘
        recent_7_avg = series.tail(7).mean()
        previous_7_avg = series.iloc[-14:-7].mean() if len(series) >= 14 else series.iloc[:7].mean()
        
        trend_change = (recent_7_avg - previous_7_avg) / previous_7_avg * 100
        
        if trend_change > 5:
            trend_direction = "ä¸Šå‡"
        elif trend_change < -5:
            trend_direction = "ä¸‹é™"
        else:
            trend_direction = "å¹³ç¨³"
        
        # æ³¢åŠ¨ç‡
        volatility = series.tail(7).std() / series.tail(7).mean() * 100
        
        print(f"æœ€è¿‘ 7 å¤©å¹³å‡å€¼ï¼š{recent_7_avg:,.0f}")
        print(f"å‰ 7 å¤©å¹³å‡å€¼ï¼š{previous_7_avg:,.0f}")
        print(f"è¶‹åŠ¿å˜åŒ–ï¼š{trend_change:+.1f}% ({trend_direction})")
        print(f"è¿‘æœŸæ³¢åŠ¨ç‡ï¼š{volatility:.1f}%")
        
        # æœ€é«˜/æœ€ä½å€¼
        max_date = series.idxmax()
        min_date = series.idxmin()
        
        print(f"æœ€é«˜å€¼ï¼š{series.max():,.0f} ({max_date.strftime('%Y-%m-%d')})")
        print(f"æœ€ä½å€¼ï¼š{series.min():,.0f} ({min_date.strftime('%Y-%m-%d')})")
        
        results[metric] = {
            'trend_direction': trend_direction,
            'trend_change': trend_change,
            'volatility': volatility,
            'max_value': series.max(),
            'max_date': str(max_date),
            'min_value': series.min(),
            'min_date': str(min_date),
            'recent_7d_avg': recent_7_avg,
            'ma7': ma7.tail(1).values[0] if not ma7.tail(1).empty else None,
            'ma14': ma14.tail(1).values[0] if not ma14.tail(1).empty else None
        }
    
    # å¯è§†åŒ–æ•°æ®
    print("\n## å¯è§†åŒ–å»ºè®®")
    print("-" * 40)
    print("å»ºè®®ç”Ÿæˆä»¥ä¸‹å›¾è¡¨:")
    print("  1. æŠ˜çº¿å›¾ï¼šæ¯æ—¥ç”¨æˆ·æ•° + 7 æ—¥ç§»åŠ¨å¹³å‡çº¿")
    print("  2. æŸ±çŠ¶å›¾ï¼šæ¯æ—¥æŠ•æ³¨é‡‘é¢")
    print("  3. é¢ç§¯å›¾ï¼šæŠ•æ³¨æ¬¡æ•°è¶‹åŠ¿")
    
    return results


def daily_anomaly_detection(df: pd.DataFrame, threshold: float = 0.7) -> dict:
    """
    æ—¥å¸¸æ•°æ®å¼‚å¸¸æ£€æµ‹
    """
    print("\n" + "="*60)
    print("ğŸ” å¼‚å¸¸ç‚¹æ£€æµ‹")
    print("="*60)
    
    results = {}
    all_anomalies = []
    
    for metric in ['user_count', 'bet_count', 'total_amount']:
        if metric not in df.columns:
            continue
        
        print(f"\n## {metric} å¼‚å¸¸æ£€æµ‹")
        print("-" * 40)
        
        series = df[metric].dropna()
        
        # Z-Score æ–¹æ³•
        mean = series.mean()
        std = series.std()
        z_scores = np.abs((series - mean) / std)
        
        # IQR æ–¹æ³•
        q1 = series.quantile(0.25)
        q3 = series.quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        # è¯†åˆ«å¼‚å¸¸ç‚¹
        zscore_anomalies = z_scores > 2.5
        iqr_anomalies = (series < lower_bound) | (series > upper_bound)
        
        # ç»¼åˆåˆ¤æ–­
        anomalies = zscore_anomalies | iqr_anomalies
        
        anomaly_dates = series.index[anomalies]
        
        if len(anomaly_dates) > 0:
            print(f"âš ï¸  å‘ç° {len(anomaly_dates)} ä¸ªå¼‚å¸¸æ—¥æœŸ:")
            for date in anomaly_dates:
                value = series.loc[date]
                z = z_scores.loc[date]
                
                # åˆ¤æ–­æ˜¯å¼‚å¸¸é«˜è¿˜æ˜¯å¼‚å¸¸ä½
                if value > upper_bound:
                    level = "å¼‚å¸¸é«˜"
                elif value < lower_bound:
                    level = "å¼‚å¸¸ä½"
                else:
                    level = "åç¦»å‡å€¼"
                
                print(f"  - {date.strftime('%Y-%m-%d')}: {value:,.0f} (Z-Score: {z:.2f}, {level})")
                
                all_anomalies.append({
                    'date': str(date),
                    'metric': metric,
                    'value': value,
                    'z_score': float(z),
                    'level': level
                })
        else:
            print("âœ… æœªå‘ç°æ˜¾è‘—å¼‚å¸¸")
        
        results[metric] = {
            'anomaly_count': len(anomaly_dates),
            'anomaly_dates': [str(d) for d in anomaly_dates],
            'zscore_threshold': 2.5,
            'iqr_bounds': (lower_bound, upper_bound)
        }
    
    results['all_anomalies'] = all_anomalies
    
    if all_anomalies:
        print(f"\nğŸ“Š æ€»è®¡å‘ç° {len(all_anomalies)} ä¸ªå¼‚å¸¸è®°å½•")
    else:
        print("\nâœ… æ‰€æœ‰æŒ‡æ ‡å‡æ— æ˜¾è‘—å¼‚å¸¸")
    
    return results


def drill_down_on_anomaly(df: pd.DataFrame, anomaly_dates: list) -> dict:
    """
    å¯¹å¼‚å¸¸æ—¥æœŸè¿›è¡Œä¸‹é’»åˆ†æ
    """
    if not anomaly_dates:
        print("\næ— éœ€ä¸‹é’»åˆ†æ (æ— å¼‚å¸¸æ—¥æœŸ)")
        return {}
    
    print("\n" + "="*60)
    print("ğŸ”¬ å¼‚å¸¸æ—¥æœŸä¸‹é’»åˆ†æ")
    print("="*60)
    
    # è½¬æ¢æ—¥æœŸå­—ç¬¦ä¸²ä¸º datetime
    anomaly_dates_dt = [pd.to_datetime(d) for d in anomaly_dates]
    
    # æ­£å¸¸æ—¥æœŸ
    normal_dates = [d for d in df.index if d not in anomaly_dates_dt]
    
    if not normal_dates:
        print("âš ï¸  æ— æ³•è·å–æ­£å¸¸æ—¥æœŸä½œä¸ºå¯¹æ¯”åŸºå‡†")
        return {}
    
    results = {}
    
    for anomaly_date in anomaly_dates_dt[:3]:  # æœ€å¤šåˆ†æå‰ 3 ä¸ªå¼‚å¸¸æ—¥æœŸ
        print(f"\n## {anomaly_date.strftime('%Y-%m-%d')} ä¸‹é’»åˆ†æ")
        print("-" * 40)
        
        # è·å–å¼‚å¸¸æ—¥æ•°æ®
        anomaly_data = df.loc[[anomaly_date]]
        
        # è·å–æ­£å¸¸æ—¥å¹³å‡æ•°æ®
        normal_avg = df.loc[normal_dates].mean()
        
        # å¯¹æ¯”åˆ†æ
        print("\nä¸æ­£å¸¸æ—¥å‡å€¼å¯¹æ¯”:")
        for metric in ['user_count', 'bet_count', 'total_amount', 'avg_amount']:
            if metric in anomaly_data.columns and metric in normal_avg.index:
                anomaly_val = anomaly_data[metric].values[0]
                normal_val = normal_avg[metric]
                change = (anomaly_val - normal_val) / normal_val * 100
                
                if abs(change) > 20:
                    flag = "âš ï¸" if change > 0 else "ğŸ”»" if change < 0 else "â¡ï¸"
                else:
                    flag = "â¡ï¸"
                
                print(f"  {flag} {metric}: {anomaly_val:,.0f} (vs æ­£å¸¸ {normal_val:,.0f}, å˜åŒ– {change:+.1f}%)")
        
        # åˆ†æå¯èƒ½çš„åŸå› 
        print("\nå¯èƒ½åŸå› åˆ†æ:")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å‘¨æœ«/å·¥ä½œæ—¥
        day_of_week = anomaly_date.day_name()
        is_weekend = anomaly_date.weekday() >= 5
        
        if is_weekend:
            print(f"  - è¯¥æ—¥æœŸæ˜¯ {day_of_week} (å‘¨æœ«)ï¼Œé€šå¸¸æµé‡è¾ƒé«˜")
        else:
            print(f"  - è¯¥æ—¥æœŸæ˜¯ {day_of_week} (å·¥ä½œæ—¥)")
        
        # æ£€æŸ¥æœˆåˆ/æœˆæœ«æ•ˆåº”
        day_of_month = anomaly_date.day
        if day_of_month <= 5:
            print(f"  - æœˆåˆ ({day_of_month}æ—¥)ï¼Œå¯èƒ½æœ‰æœˆåˆæ•ˆåº”")
        elif day_of_month >= 25:
            print(f"  - æœˆæœ« ({day_of_month}æ—¥)ï¼Œå¯èƒ½æœ‰æœˆæœ«æ•ˆåº”")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯èŠ‚å‡æ—¥ (ç®€åŒ–ç‰ˆ)
        # å®é™…åº”ç”¨ä¸­åº”è¯¥æ¥å…¥èŠ‚å‡æ—¥ API
        
        results[str(anomaly_date)] = {
            'day_of_week': day_of_week,
            'is_weekend': is_weekend,
            'day_of_month': day_of_month,
            'metrics_comparison': {
                metric: {
                    'anomaly_value': float(anomaly_data[metric].values[0]),
                    'normal_avg': float(normal_avg[metric]),
                    'change_pct': float((anomaly_data[metric].values[0] - normal_avg[metric]) / normal_avg[metric] * 100)
                }
                for metric in ['user_count', 'bet_count', 'total_amount']
                if metric in anomaly_data.columns
            }
        }
    
    return results


def generate_summary_report(trend_results: dict, anomaly_results: dict, 
                           drilldown_results: dict, output_path: str = None):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    report = []
    report.append("# ODPS æŠ•æ³¨æ•°æ®æ—¥å¸¸åˆ†ææŠ¥å‘Š")
    report.append("")
    report.append(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # è¶‹åŠ¿æ‘˜è¦
    report.append("## ğŸ“ˆ è¶‹åŠ¿æ‘˜è¦")
    report.append("")
    
    for metric, data in trend_results.items():
        if metric in ['user_count', 'bet_count', 'total_amount']:
            report.append(f"### {metric}")
            report.append(f"- è¶‹åŠ¿æ–¹å‘ï¼š{data['trend_direction']}")
            report.append(f"- è¶‹åŠ¿å˜åŒ–ï¼š{data['trend_change']:+.1f}%")
            report.append(f"- è¿‘æœŸæ³¢åŠ¨ç‡ï¼š{data['volatility']:.1f}%")
            report.append(f"- æœ€è¿‘ 7 å¤©å‡å€¼ï¼š{data['recent_7d_avg']:,.0f}")
            report.append("")
    
    # å¼‚å¸¸æ‘˜è¦
    report.append("## ğŸ” å¼‚å¸¸æ£€æµ‹æ‘˜è¦")
    report.append("")
    
    all_anomalies = anomaly_results.get('all_anomalies', [])
    
    if all_anomalies:
        report.append(f"**å‘ç° {len(all_anomalies)} ä¸ªå¼‚å¸¸è®°å½•**:")
        report.append("")
        report.append("| æ—¥æœŸ | æŒ‡æ ‡ | æ•°å€¼ | Z-Score | ç±»å‹ |")
        report.append("|------|------|------|---------|------|")
        
        for anomaly in all_anomalies:
            report.append(f"| {anomaly['date']} | {anomaly['metric']} | {anomaly['value']:,.0f} | {anomaly['z_score']:.2f} | {anomaly['level']} |")
    else:
        report.append("âœ… æ‰€æœ‰æŒ‡æ ‡å‡æ— æ˜¾è‘—å¼‚å¸¸")
    
    report.append("")
    
    # ä¸‹é’»åˆ†ææ‘˜è¦
    if drilldown_results:
        report.append("## ğŸ”¬ ä¸‹é’»åˆ†ææ‘˜è¦")
        report.append("")
        
        for date, data in drilldown_results.items():
            report.append(f"### {date}")
            report.append(f"- æ˜ŸæœŸï¼š{data['day_of_week']} ({'å‘¨æœ«' if data['is_weekend'] else 'å·¥ä½œæ—¥'})")
            report.append(f"- æ—¥æœŸï¼š{data['day_of_month']}æ—¥")
            report.append("")
            report.append("| æŒ‡æ ‡ | å¼‚å¸¸å€¼ | æ­£å¸¸å‡å€¼ | å˜åŒ– |")
            report.append("|------|--------|---------|------|")
            
            for metric, comparison in data['metrics_comparison'].items():
                change = comparison['change_pct']
                arrow = "â†‘" if change > 0 else "â†“" if change < 0 else "â†’"
                report.append(f"| {metric} | {comparison['anomaly_value']:,.0f} | {comparison['normal_avg']:,.0f} | {arrow} {change:+.1f}% |")
            
            report.append("")
    
    # å»ºè®®
    report.append("## ğŸ’¡ å»ºè®®")
    report.append("")
    
    if all_anomalies:
        report.append("1. **è°ƒæŸ¥å¼‚å¸¸æ—¥æœŸ**: æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Šäº‹ä»¶ã€ç³»ç»Ÿé—®é¢˜æˆ–æ•°æ®è´¨é‡é—®é¢˜")
        report.append("2. **ç›‘æ§è¶‹åŠ¿å˜åŒ–**: å¦‚æœè¶‹åŠ¿æŒç»­ä¸Šå‡/ä¸‹é™ï¼Œéœ€è¦åˆ†æåŸå› ")
        report.append("3. **å»ºç«‹é¢„è­¦æœºåˆ¶**: å¯¹å¼‚å¸¸æ³¢åŠ¨è®¾ç½®è‡ªåŠ¨å‘Šè­¦")
    else:
        report.append("1. **æŒç»­ç›‘æ§**: ä¿æŒæ—¥å¸¸æ•°æ®ç›‘æ§")
        report.append("2. **å»ºç«‹åŸºçº¿**: åŸºäºå†å²æ•°æ®å»ºç«‹æ­£å¸¸æ³¢åŠ¨èŒƒå›´")
        report.append("3. **å®šæœŸåˆ†æ**: å»ºè®®æ¯å‘¨/æ¯æœˆè¿›è¡Œæ·±åº¦åˆ†æ")
    
    report_text = '\n'.join(report)
    
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        print(f"\nğŸ“ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜ï¼š{output_path}")
    
    return report_text


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ODPS æŠ•æ³¨æ•°æ®æ—¥å¸¸åˆ†æ')
    parser.add_argument('--project', default=None, help='ODPS é¡¹ç›®åç§°')
    parser.add_argument('--days', type=int, default=30, help='æŸ¥è¯¢å¤©æ•° (é»˜è®¤ 30)')
    parser.add_argument('--output', default='reports/betting_daily_analysis', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--sample', action='store_true', help='ä½¿ç”¨ç¤ºä¾‹æ•°æ® (æµ‹è¯•ç”¨)')
    parser.add_argument('--anomaly-threshold', type=float, default=0.7, help='å¼‚å¸¸æ£€æµ‹é˜ˆå€¼')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®æˆ–ä½¿ç”¨ç¤ºä¾‹æ•°æ®
    if args.sample:
        print("ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œåˆ†æ...\n")
        df = generate_sample_daily_data(args.days)
    else:
        if not check_odps_config():
            print("\nåˆ‡æ¢åˆ°ç¤ºä¾‹æ•°æ®æ¨¡å¼...")
            df = generate_sample_daily_data(args.days)
        else:
            project = args.project or os.getenv('ALIBABA_ODPS_PROJECT')
            if not project:
                print("è¯·æŒ‡å®š ODPS é¡¹ç›®åç§° (--project) æˆ–è®¾ç½® ALIBABA_ODPS_PROJECT ç¯å¢ƒå˜é‡")
                sys.exit(1)
            
            df = query_odps_daily_data(project, args.days)
            
            if df is None or len(df) == 0:
                print("æŸ¥è¯¢å¤±è´¥æˆ–æ— æ•°æ®ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®...")
                df = generate_sample_daily_data(args.days)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # 1. è¶‹åŠ¿åˆ†æ
    trend_results = daily_trend_analysis(df)
    
    # 2. å¼‚å¸¸æ£€æµ‹
    anomaly_results = daily_anomaly_detection(df, args.anomaly_threshold)
    
    # 3. ä¸‹é’»åˆ†æ (å¦‚æœæœ‰å¼‚å¸¸)
    anomaly_dates = anomaly_results.get('all_anomalies', [])
    anomaly_date_list = [a['date'] for a in anomaly_dates]
    drilldown_results = drill_down_on_anomaly(df, anomaly_date_list)
    
    # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    report = generate_summary_report(
        trend_results, 
        anomaly_results, 
        drilldown_results,
        output_path / 'daily_analysis_report.md'
    )
    
    # ä¿å­˜è¯¦ç»†æ•°æ®
    df.to_csv(output_path / 'daily_data.csv')
    print(f"ğŸ“ è¯¦ç»†æ•°æ®å·²ä¿å­˜ï¼š{output_path / 'daily_data.csv'}")
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“‹ ç»¼åˆåˆ†ææŠ¥å‘Š")
    print("="*60)
    print(report)
    
    print("\n" + "="*60)
    print("âœ… åˆ†æå®Œæˆ!")
    print("="*60)


if __name__ == '__main__':
    main()
