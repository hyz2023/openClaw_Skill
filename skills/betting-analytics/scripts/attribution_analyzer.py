#!/usr/bin/env python3
"""
åšå½©æ•°æ®å½’å› åˆ†æå·¥å…·
Attribution Analyzer for Betting Data

åŠŸèƒ½:
- ç‰¹å¾é‡è¦æ€§åˆ†æ
- SHAP å€¼è§£é‡Š
- è´¡çŒ®åº¦åˆ†è§£
- å› æœæ¨æ–­
- å½±å“å› ç´ æ’åº
"""

import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score


def load_data(data_path: str) -> pd.DataFrame:
    """åŠ è½½æ•°æ®æ–‡ä»¶"""
    path = Path(data_path)
    if path.suffix == '.csv':
        df = pd.read_csv(data_path)
    elif path.suffix == '.json':
        df = pd.read_json(data_path)
    else:
        raise ValueError(f"Unsupported file format: {path.suffix}")
    
    return df


def encode_categorical_features(df: pd.DataFrame, 
                                 exclude_cols: list = None) -> tuple:
    """
    ç¼–ç åˆ†ç±»ç‰¹å¾
    
    Returns:
        encoded_df: ç¼–ç åçš„ DataFrame
        encoders: ç¼–ç å™¨å­—å…¸
    """
    if exclude_cols is None:
        exclude_cols = []
    
    encoded_df = df.copy()
    encoders = {}
    
    for col in df.columns:
        if col in exclude_cols:
            continue
        
        if df[col].dtype == 'object' or isinstance(df[col].dtype, pd.CategoricalDtype):
            le = LabelEncoder()
            # å¤„ç† NaN å€¼
            mask = df[col].notna()
            encoded_df.loc[mask, col] = le.fit_transform(df.loc[mask, col].astype(str))
            encoded_df[col] = encoded_df[col].fillna(-1).astype(int)
            encoders[col] = le
    
    return encoded_df, encoders


def calculate_feature_importance(X: pd.DataFrame, 
                                  y: pd.Series,
                                  method: str = 'random_forest',
                                  n_estimators: int = 100) -> pd.Series:
    """
    è®¡ç®—ç‰¹å¾é‡è¦æ€§
    
    Args:
        X: ç‰¹å¾ DataFrame
        y: ç›®æ ‡å˜é‡
        method: æ–¹æ³• (random_forest / gradient_boosting)
        n_estimators: æ ‘çš„æ•°é‡
    
    Returns:
        ç‰¹å¾é‡è¦æ€§ Series
    """
    # å¤„ç†ç¼ºå¤±å€¼
    X_clean = X.fillna(X.median(numeric_only=True))
    
    if method == 'random_forest':
        model = RandomForestClassifier(
            n_estimators=n_estimators,
            random_state=42,
            n_jobs=-1,
            max_depth=10
        )
    elif method == 'gradient_boosting':
        model = GradientBoostingClassifier(
            n_estimators=n_estimators,
            random_state=42,
            max_depth=5
        )
    else:
        raise ValueError(f"Unknown method: {method}")
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_clean, y)
    
    # è·å–ç‰¹å¾é‡è¦æ€§
    importance = pd.Series(
        model.feature_importances_,
        index=X.columns,
        name='importance'
    )
    
    # æ’åº
    importance = importance.sort_values(ascending=False)
    
    return importance


def calculate_shap_values(X: pd.DataFrame, 
                          y: pd.Series,
                          n_samples: int = 100) -> dict:
    """
    è®¡ç®— SHAP å€¼ (ç®€åŒ–ç‰ˆæœ¬)
    
    æ³¨æ„ï¼šå®Œæ•´ SHAP éœ€è¦ shap åº“ï¼Œè¿™é‡Œæä¾›ç®€åŒ–å®ç°
    """
    try:
        import shap
        has_shap = True
    except ImportError:
        has_shap = False
        print("è­¦å‘Šï¼šshap åº“æœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ– SHAP è¿‘ä¼¼")
    
    # å¤„ç†ç¼ºå¤±å€¼
    X_clean = X.fillna(X.median(numeric_only=True))
    
    # è®­ç»ƒæ¨¡å‹
    model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
    model.fit(X_clean, y)
    
    # é‡‡æ ·
    if len(X_clean) > n_samples:
        X_sample = X_clean.sample(n=n_samples, random_state=42)
    else:
        X_sample = X_clean
    
    if has_shap:
        # ä½¿ç”¨çœŸæ­£çš„ SHAP
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_sample)
        
        # å¯¹äºå¤šåˆ†ç±»ï¼Œå–å¹³å‡ç»å¯¹å€¼
        if isinstance(shap_values, list):
            shap_summary = np.mean([np.abs(sv) for sv in shap_values], axis=0)
        else:
            shap_summary = np.abs(shap_values)
        
        shap_importance = pd.Series(
            shap_summary.mean(axis=0),
            index=X.columns,
            name='shap_importance'
        )
    else:
        # ç®€åŒ– SHAP è¿‘ä¼¼ (åŸºäºæ’åˆ—é‡è¦æ€§)
        base_score = model.score(X_clean, y)
        shap_scores = []
        
        for col in X.columns:
            X_permuted = X_clean.copy()
            X_permuted[col] = np.random.permutation(X_permuted[col])
            permuted_score = model.score(X_permuted, y)
            score_drop = base_score - permuted_score
            shap_scores.append(max(0, score_drop))
        
        shap_importance = pd.Series(
            shap_scores,
            index=X.columns,
            name='shap_importance'
        )
    
    shap_importance = shap_importance.sort_values(ascending=False)
    
    return {
        'importance': shap_importance,
        'model': model
    }


def decompose_contribution(X: pd.DataFrame,
                           y: pd.Series,
                           target_value: float = None) -> pd.DataFrame:
    """
    åˆ†è§£æ¯ä¸ªæ ·æœ¬çš„è´¡çŒ®åº¦
    
    Returns:
        è´¡çŒ®åº¦ DataFrame
    """
    # å¤„ç†ç¼ºå¤±å€¼
    X_clean = X.fillna(X.median(numeric_only=True))
    
    # è®­ç»ƒæ¨¡å‹
    model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
    model.fit(X_clean, y)
    
    # è®¡ç®—åŸºå‡†é¢„æµ‹
    base_pred = model.predict_proba(X_clean)[:, 1].mean()
    
    # å¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œè®¡ç®—ç§»é™¤è¯¥ç‰¹å¾åçš„é¢„æµ‹å˜åŒ–
    contributions = pd.DataFrame(index=X.index)
    contributions['base_prediction'] = base_pred
    contributions['actual_prediction'] = model.predict_proba(X_clean)[:, 1]
    
    for col in X.columns:
        X_modified = X_clean.copy()
        # å°†è¯¥ç‰¹å¾æ›¿æ¢ä¸ºä¸­ä½æ•°
        X_modified[col] = X_clean[col].median()
        
        modified_pred = model.predict_proba(X_modified)[:, 1]
        contribution = contributions['actual_prediction'] - modified_pred
        
        contributions[col] = contribution
    
    contributions['total_contribution'] = contributions['actual_prediction'] - contributions['base_prediction']
    
    return contributions


def analyze_attribution(df: pd.DataFrame,
                        target_col: str,
                        method: str = 'shap',
                        exclude_cols: list = None) -> dict:
    """
    æ‰§è¡Œå½’å› åˆ†æ
    
    Args:
        df: æ•°æ® DataFrame
        target_col: ç›®æ ‡å˜é‡åˆ—
        method: åˆ†ææ–¹æ³• (feature_importance / shap / decomposition)
        exclude_cols: æ’é™¤çš„åˆ—
    
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    if exclude_cols is None:
        exclude_cols = []
    
    # æ·»åŠ ç›®æ ‡å˜é‡åˆ°æ’é™¤åˆ—è¡¨
    if target_col not in exclude_cols:
        exclude_cols.append(target_col)
    
    # ç¼–ç åˆ†ç±»ç‰¹å¾
    print("ç¼–ç åˆ†ç±»ç‰¹å¾...")
    encoded_df, encoders = encode_categorical_features(df, exclude_cols)
    
    # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
    feature_cols = [col for col in encoded_df.columns if col not in exclude_cols]
    feature_cols = [col for col in feature_cols if encoded_df[col].dtype in [np.number, int]]
    
    X = encoded_df[feature_cols]
    y = encoded_df[target_col]
    
    # å¤„ç†ç›®æ ‡å˜é‡ (å¦‚æœæ˜¯å¤šåˆ†ç±»ï¼Œè½¬ä¸ºäºŒåˆ†ç±»)
    if y.nunique() > 2:
        print(f"ç›®æ ‡å˜é‡æœ‰ {y.nunique()} ä¸ªç±»åˆ«ï¼Œè½¬ä¸ºäºŒåˆ†ç±» (>=ä¸­ä½æ•°)")
        y = (y >= y.median()).astype(int)
    
    # å¤„ç†ç¼ºå¤±å€¼
    X = X.fillna(X.median(numeric_only=True))
    
    print(f"ç‰¹å¾æ•°é‡ï¼š{len(feature_cols)}")
    print(f"æ ·æœ¬æ•°é‡ï¼š{len(X)}")
    
    # æ‰§è¡Œåˆ†æ
    if method == 'feature_importance':
        print("è®¡ç®—ç‰¹å¾é‡è¦æ€§ (Random Forest)...")
        importance = calculate_feature_importance(X, y, method='random_forest')
        results = {
            'method': 'feature_importance',
            'importance': importance,
            'model_type': 'Random Forest'
        }
    
    elif method == 'shap':
        print("è®¡ç®— SHAP å€¼...")
        shap_results = calculate_shap_values(X, y)
        results = {
            'method': 'shap',
            'importance': shap_results['importance'],
            'model': shap_results['model'],
            'model_type': 'Random Forest + SHAP'
        }
    
    elif method == 'decomposition':
        print("è®¡ç®—è´¡çŒ®åº¦åˆ†è§£...")
        contributions = decompose_contribution(X, y)
        importance = calculate_feature_importance(X, y)
        results = {
            'method': 'decomposition',
            'importance': importance,
            'contributions': contributions,
            'model_type': 'Random Forest'
        }
    
    else:
        raise ValueError(f"Unknown method: {method}")
    
    # æ¨¡å‹è¯„ä¼°
    from sklearn.model_selection import cross_val_score
    model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    
    results['model_performance'] = {
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'cv_scores': cv_scores.tolist()
    }
    
    # æ·»åŠ ç‰¹å¾ç»Ÿè®¡
    results['feature_stats'] = {}
    for col in feature_cols[:10]:  # åªæ˜¾ç¤ºå‰ 10 ä¸ªç‰¹å¾
        results['feature_stats'][col] = {
            'type': str(X[col].dtype),
            'unique_values': X[col].nunique(),
            'missing_rate': X[col].isna().sum() / len(X)
        }
    
    return results


def generate_report(results: dict, output_path: str = None) -> str:
    """ç”Ÿæˆå½’å› åˆ†ææŠ¥å‘Š"""
    report = []
    report.append("## ğŸ¯ å½’å› åˆ†ææŠ¥å‘Š")
    report.append("")
    report.append(f"**åˆ†ææ–¹æ³•**: {results['method']}")
    report.append(f"**æ¨¡å‹ç±»å‹**: {results['model_type']}")
    report.append("")
    
    # æ¨¡å‹æ€§èƒ½
    perf = results['model_performance']
    report.append("### æ¨¡å‹æ€§èƒ½")
    report.append(f"| æŒ‡æ ‡ | æ•°å€¼ |")
    report.append(f"|------|------|")
    report.append(f"| äº¤å‰éªŒè¯å‡†ç¡®ç‡ | {perf['cv_mean']:.2%} |")
    report.append(f"| æ ‡å‡†å·® | {perf['cv_std']:.2%} |")
    report.append("")
    
    # ç‰¹å¾é‡è¦æ€§
    report.append("### ç‰¹å¾é‡è¦æ€§ (Top 10)")
    report.append("")
    importance = results['importance']
    
    report.append("| æ’å | ç‰¹å¾ | é‡è¦æ€§ | å æ¯” |")
    report.append("|------|------|--------|------|")
    
    total_importance = importance.sum()
    for i, (feature, imp) in enumerate(importance.head(10).items(), 1):
        percentage = (imp / total_importance * 100) if total_importance > 0 else 0
        # å¯è§†åŒ–æ¡
        bar_len = int(percentage / 5)
        bar = "â–ˆ" * bar_len
        report.append(f"| {i} | {feature} | {imp:.4f} | {percentage:.1f}% {bar} |")
    
    report.append("")
    
    # å…³é”®å‘ç°
    report.append("### å…³é”®å‘ç°")
    report.append("")
    
    top_3 = importance.head(3)
    if len(top_3) > 0:
        report.append(f"**Top 3 å½±å“å› ç´ **:")
        for feature, imp in top_3.items():
            report.append(f"1. **{feature}**: é‡è¦æ€§ {imp:.4f} ({imp/total_importance*100:.1f}%)")
        report.append("")
    
    # ç´¯è®¡è´¡çŒ®
    cumsum = importance.cumsum() / total_importance * 100
    top_5_idx = cumsum.head(5).index.tolist()
    top_5_coverage = cumsum.head(5).values[-1] if len(cumsum) >= 5 else cumsum.values[-1]
    
    report.append(f"**å‰ 5 å¤§ç‰¹å¾ç´¯è®¡è´¡çŒ®**: {top_5_coverage:.1f}%")
    report.append("")
    
    # å»ºè®®
    report.append("### å»ºè®®")
    report.append("")
    
    if perf['cv_mean'] > 0.8:
        report.append("âœ… æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼Œç‰¹å¾é‡è¦æ€§å¯ä¿¡åº¦é«˜")
    elif perf['cv_mean'] > 0.6:
        report.append("âš¡ æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå¯å‚è€ƒç‰¹å¾é‡è¦æ€§è¿›è¡Œåˆ†æ")
    else:
        report.append("âš ï¸ æ¨¡å‹æ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®:")
        report.append("1. å¢åŠ æ›´å¤šç‰¹å¾")
        report.append("2. æ”¶é›†æ›´å¤šæ ·æœ¬æ•°æ®")
        report.append("3. å°è¯•å…¶ä»–æ¨¡å‹")
    
    report.append("")
    
    # å¦‚æœæœ‰è´¡çŒ®åº¦åˆ†è§£
    if 'contributions' in results:
        report.append("### æ ·æœ¬è´¡çŒ®åº¦ç¤ºä¾‹")
        report.append("")
        contrib = results['contributions'].head(5)
        report.append("å‰ 5 ä¸ªæ ·æœ¬çš„é¢„æµ‹è´¡çŒ®åº¦:")
        report.append(contrib.to_string())
        report.append("")
    
    report_text = '\n'.join(report)
    
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
    
    return report_text


def main():
    parser = argparse.ArgumentParser(description='åšå½©æ•°æ®å½’å› åˆ†æå·¥å…·')
    parser.add_argument('--data', required=True, help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ (CSV/JSON)')
    parser.add_argument('--target', required=True, help='ç›®æ ‡å˜é‡åˆ—å')
    parser.add_argument('--method', default='shap',
                        choices=['feature_importance', 'shap', 'decomposition'],
                        help='åˆ†ææ–¹æ³•')
    parser.add_argument('--exclude', nargs='+', default=None, 
                        help='æ’é™¤çš„åˆ—ååˆ—è¡¨')
    parser.add_argument('--output', default=None, help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®
    print(f"åŠ è½½æ•°æ®ï¼š{args.data}")
    df = load_data(args.data)
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
    
    # æ£€æŸ¥ç›®æ ‡åˆ—
    if args.target not in df.columns:
        print(f"é”™è¯¯ï¼šç›®æ ‡åˆ— '{args.target}' ä¸å­˜åœ¨")
        print(f"å¯ç”¨åˆ—ï¼š{list(df.columns)}")
        return
    
    # æ‰§è¡Œå½’å› åˆ†æ
    print("æ‰§è¡Œå½’å› åˆ†æ...")
    results = analyze_attribution(
        df, 
        args.target, 
        args.method,
        args.exclude
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(results, args.output)
    print("\n" + report)
    
    if args.output:
        print(f"\næŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{args.output}")


if __name__ == '__main__':
    main()
